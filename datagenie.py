# -*- coding: utf-8 -*-
"""DataGenie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ikIvLnhtpsnt_BfRFfqJ7ckVYRayFxlz

# **DataGenie â€“ Autonomous AI Data Analyst Agent**

1. PROJECT ABSTRACT

DataGenie â€“ Autonomous AI Data Analyst Agent is an end-to-end intelligent system designed to automate the entire data analysis lifecycle.
The system accepts raw datasets in formats such as CSV, Excel, or Google Sheets, performs comprehensive Exploratory Data Analysis (EDA), cleans and preprocesses data, trains multiple machine learning models, evaluates them, and generates business insights using LLM-powered reasoning.
The agent combines classical machine learning techniques with agentic AI planning, enabling it to autonomously select the right steps to produce accurate predictions and clear reports â€” without requiring any coding from the user.
This project aims to reduce manual data cleaning and analysis work, making data insights accessible to non-technical users and small teams.

**1.Input Handling System**

Goal: Make system accept ANY dataset.

Tasks

 Build CSV loader

 Build Excel loader

 Build Google Sheets loader

 Detect:

missing values

duplicates

data types

zero values

unique values

 Standardize all outputs into a single structure

 Save as reusable functions

 Test with 5 datasets

Load CSV, Excel, Google Sheets
"""

import pandas as pd
import numpy as np

def load_data(source):
  """Load CSV, Excel, or Google Sheets link."""

  if "docs.google.com" in source:
    csv_url = source.replace("/edit#gid=", "/export?format=csv&gid=")
    return pd.read_csv(csv_url)

  if source.endswith('xlsx') or source.endswith('xls'):
    return pd.read_excel(source)

  if source.endswith('.csv'):
    return pd.read_csv(source)

  raise ValueError("Unsupported file format or URL.")

"""Analyze structure"""

def analyse_structure(data):
  info={
      "shape": data.shape,
      "columns": data.columns.tolist(),
      "dtypes": data.dtypes.to_dict(),
      "missing_values": data.isnull().sum().to_dict(),
      "zero_values": (data==0).sum().to_dict(),
      "unique_counts": data.nunique().to_dict(),
      "duplicates": data.duplicated().sum(),
      "sample_row": data.head(10).to_dict(orient="records")
  }
  return info

"""Combine Input Processor"""

def process_input(source):
  data=load_data(source)
  structure = analyse_structure(data)
  return data,structure

"""Test Pipeline"""

data, structure = process_input("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv")
data
structure

"""**2. EDA Core Functions**

Detect Column Types
"""

def detect_column_types(data):
  numeric_cols=data.select_dtypes(include=['int64','float64']).columns.tolist()
  categorical_cols=data.select_dtypes(include=['object','category']).columns.tolist()
  return numeric_cols,categorical_cols

"""Summary statistics"""

def summary_stats(data):
  numeric_cols, categorical_cols = detect_column_types(data)

  summary = {
      "numeric_summary":data[numeric_cols].describe().to_dict(),
      "categorical_summary":{col:data[col].value_counts().to_dict() for col in categorical_cols}
  }
  return summary

"""Correlation matrix"""

def correlation_matrix(data):
  numerical_cols,_=detect_column_types(data)
  corr_matrix=data[numerical_cols].corr().round(3).to_dict()
  return corr_matrix

"""Outlier detection (IQR)"""

def outlier_report(data):
  numeric_cols, _ = detect_column_types(data)
  outlier = {}

  for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1

    Lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outlier[col] = int(((data[col] < Lower_bound) | (data[col] > upper_bound)).sum())
  return outlier

"""Full EDA engine"""

def run_eda(data):
  eda={
  'column_types':detect_column_types(data),
  'summary_stats':summary_stats(data),
  'correlaton_matrix':correlation_matrix(data),
  'outlier_report':outlier_report(data)
  }
  print("EDA Completed")
  return eda

"""Test EDA Engine:"""

eda_result=run_eda(data)
eda_result

"""**3.EDA Plot Functions**"""

import os
os.makedirs("plots",exist_ok=True)

"""Distribution plots"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_distributions(data):
  numeric_col, _ = detect_column_types(data)
  for col in numeric_col:
    plt.figure(figsize=(6,4)) # Corrected keyword: figsize
    sns.histplot(data[col], kde=True) # Corrected column access: data[col]
    plt.title(f"Distribuition - {col}")
    plt.savefig(f"plots/dist_{col}.png")
    plt.close()

"""Boxplots"""

def plot_boxplots(df):
    numeric_col, _ = detect_column_types(df)
    for col in numeric_col:
        plt.figure(figsize=(6,4))
        sns.boxplot(x=df[col])
        plt.title(f"Boxplot - {col}")
        plt.savefig(f"plots/box_{col}.png")
        plt.close()

"""Correlation heatmap"""

def plot_heatmap(df):
    numeric_col, _ = detect_column_types(df)
    plt.figure(figsize=(8,6))
    sns.heatmap(df[numeric_col].corr(), annot=True, cmap="coolwarm")
    plt.title("Correlation Heatmap")
    plt.savefig("plots/corr_heatmap.png")
    plt.close()

"""Missing value heatmap"""

def plot_missing_values(df):
    plt.figure(figsize=(8,6))
    sns.heatmap(df.isnull(), cbar=False)
    plt.title("Missing Value Heatmap")
    plt.savefig("plots/missing_heatmap.png")
    plt.close()

"""Run ALL plots"""

def generate_all_plots(data):
  plot_distributions(data)
  plot_boxplots(data)
  plot_heatmap(data)
  plot_missing_values(data)
  print("All plots generated (saved in /plots folder)")

"""Test Plots"""

generate_all_plots(data)

"""**4.Data Cleaning & Preprocessing Pipeline**

Missing value handling
"""

def fill_missing_value(data):
  data_clean=data.copy()
  strategy={}

  for col in data_clean.columns:
    if data_clean[col].isnull().sum()>0:

      #Numeric columns - mean/median
      if data_clean[col].dtype in['int64','float64']:
        value = data_clean[col].median()
        data_clean[col].fillna(value, inplace=True)
        strategy[col] = f"median ({value})"

      #Categorical - mode
      else:
        value = data_clean[col].mode()
        data_clean[col].fillna(value, inplace=True)
        strategy[col] = f"mode ({value})"

  return data_clean, strategy

"""Encoding Categorical Columns"""

from sklearn.preprocessing import LabelEncoder

def encode_categorical(data):
  data_encoder = data.copy()
  encoder={}

  for col in data_encoder.select_dtypes(include=['object','category']):

    unique_vals = data_encoder[col].nunique()

    #Label Encode for small cardinality (eg: ..<= 10)
    if unique_vals <=10:
      le = LabelEncoder()
      data_encoder[col] = le.fit_transform(data_encoder[col])
      encoder[col] = {"type":"label","mapping":dict(zip(le.classes_, le.transform(le.classes_)))}

    # One-hot encode for large cardinality columns:

    else:
      data_encoder = pd.get_dummies(data_encoder, columns=[col])
      encoder[col] = {"type": "one-hot"}

  return data_encoder, encoder

"""Scaling Numeric Columns"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

def scale_numeric(data, method='standard'):
    data_scaled = data.copy()
    numeric_cols = data_scaled.select_dtypes(include=['int64','float64']).columns.tolist()

    if method == "standard":
        scaler = StandardScaler()
    else:
        scaler = MinMaxScaler()

    data_scaled[numeric_cols] = scaler.fit_transform(data_scaled[numeric_cols])

    return data_scaled, scaler, numeric_cols

"""Outlier Treatment Combined Pipeline"""

def fix_outliers(data):
  data_outlier = data.copy()
  numeric_cols = data_outlier.select_dtypes(include=['int64','float64']).columns

  for col in numeric_cols:
    Q1 = data_outlier[col].quantile(0.25)
    Q3 = data_outlier[col].quantile(0.75)

    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR # Corrected: should be Q3 + 1.5 * IQR

    data_outlier[col] = np.clip(data_outlier[col], lower_bound, upper_bound)
  return data_outlier

"""Combined Pipeline"""

# def full_preprocessing_pipeline(data, target_column):
#   steps={}
#   #missing values:
#   data_clean, missing_strategy = fill_missing_value(data)
#   steps['missing_values'] = missing_strategy

#   #Outliers:
#   data_no_outlier = fix_outliers(data_clean)
#   steps["outliers"] = "IQR clipping"

#   #Encoding:
#   data_encoded, encoders = encode_categorical(data_no_outlier)
#   steps["encoding"] = encoders

#   #Scaling
#   data_scaled, scaler, numeric_cols = scale_numeric(data_encoded, method="standard")
#   steps["scaling"] = {"method": "standard", "scaled_columns": numeric_cols}

#   print("Preprocessing Completed")

#   #Separate X and y
#   X = data_scaled.drop(columns=[target_column])
#   y = data_scaled[target_column]

#   #combining all artifacts
#   artifacts = {
#       "steps": steps,
#       "scaler":scaler,
#       "encoders":encoders
#   }

#   return X, y, artifacts

"""Test the entire pipeline"""

# target_column_name = 'tip' # Assuming 'tip' is the target for prediction based on the tips dataset.
# processed_data, target_variable, artifacts = full_preprocessing_pipeline(data, target_column_name)

# print("Processed Data Head:")
# print(processed_data.head())
# print("\nTarget Variable Head:")
# print(target_variable.head())

"""**5. AUTO ML**

imports
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.svm import SVC,SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score

import joblib
import os

"""Detect Task Type"""

def detect_task_type(data, target):
    if data[target].dtype == "object" or data[target].nunique() <= 20:
        return "classification"
    return "regression"

"""full_preprocessing_pipeline"""

def full_preprocessing_pipeline(data, target_column):

    X = data.drop(columns=[target_column])
    y = data[target_column]   # â— RAW TARGET â€” DO NOT TOUCH

    numeric_cols = X.select_dtypes(include=["int64", "float64"]).columns
    categorical_cols = X.select_dtypes(include=["object", "category"]).columns

    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline

    numeric_pipeline = Pipeline([
        ("scaler", StandardScaler())
    ])

    categorical_pipeline = Pipeline([
        ("encoder", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer([
        ("num", numeric_pipeline, numeric_cols),
        ("cat", categorical_pipeline, categorical_cols)
    ])

    X_processed = preprocessor.fit_transform(X)

    feature_names = []

    if len(numeric_cols) > 0:
        feature_names.extend(numeric_cols)

    if len(categorical_cols) > 0:
        ohe = preprocessor.named_transformers_["cat"]["encoder"]
        ohe_features = ohe.get_feature_names_out(categorical_cols)
        feature_names.extend(ohe_features)

    X_processed = pd.DataFrame(
        X_processed.toarray() if hasattr(X_processed, "toarray") else X_processed,
        columns=feature_names
    )

    artifacts = {
        "preprocessor": preprocessor,
        "numeric_cols": numeric_cols,
        "categorical_cols": categorical_cols
    }

    print("Preprocessing Completed")
    return X_processed, y, artifacts

"""Model List"""

def get_models(task):
  if task == "classification":
    return {
        "LogisticRegression": LogisticRegression(max_iter=1000),
        "DecisionTree": DecisionTreeClassifier(),
        "RandomForest": RandomForestClassifier(),
        "SVC":SVC(probability=True),
        "KNN":KNeighborsClassifier(),
        "GradientBoosting":GradientBoostingClassifier()
    }
  else:
    return {
        "LinearRegression": LinearRegression(),
        "DecisionTreeRegressor": DecisionTreeRegressor(),
        "RandomForestRegressor": RandomForestRegressor(),
        "SVR": SVR(),
        "KNNRegressor": KNeighborsRegressor(),
        "GradientBoostingRegressor": GradientBoostingRegressor()
    }

"""Metrics"""

def eval_classification(y_true, y_pred):
  return {
      "accuracy": accuracy_score(y_true, y_pred),
      "f1":f1_score(y_true, y_pred, average='weighted')
  }

def eval_regression(y_true, y_pred):
  return {
      "r2": r2_score(y_true, y_pred),
      "rmse": np.sqrt(mean_squared_error(y_true, y_pred))
    }

"""Train Single Model"""

def train_single_model(model, X_train, y_train, X_test, y_test, task):

    if task == "classification":
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring="f1_weighted")
    else:
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring="r2")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    metrics = eval_classification(y_test, y_pred) if task == "classification" else eval_regression(y_test, y_pred)
    metrics["cv_score"] = cv_score.mean()

    return metrics

"""AutoML Loop"""

def run_automl(X_train, X_test, y_train, y_test, task):
  models = get_models(task)
  results = []

  for name, model in models.items():
    metrics = train_single_model(model, X_train, y_train, X_test, y_test, task)
    results.append({"model":name, **metrics})

  return pd.DataFrame(results)

"""Choose Best Model"""

def choose_best_model(results, task):
  if task == "classification":
    return results.sort_values("cv_score", ascending=False).iloc[0]
  else:
    return results.sort_values("cv_score", ascending=False).iloc[0]

"""Save Model"""

os.makedirs("models",exist_ok=True)

def save_model(model, name="best_model"):
  joblib.dump(model, f"models/{name}.pkl")
  print(f"Model Saved: models/{name}.pkl")

"""SHAP"""

!pip install shap

import shap

def svc_class_wrapper(model, class_idx):
    def f(X):
        return model.decision_function(X)[:, class_idx]
    return f

def explain_model_with_shap(model, X_train):
    import shap
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    if not isinstance(X_train, pd.DataFrame):
        raise ValueError("X_train must be a DataFrame with feature names")

    model_name = model.__class__.__name__
    print(f"Running SHAP for model: {model_name}")

    # background data
    X_background = X_train.sample(min(50, len(X_train)), random_state=42)

    # ---------------- LINEAR MODELS ----------------
    if model_name in ["LinearRegression", "LogisticRegression"]:
        explainer = shap.LinearExplainer(model, X_background)
        shap_values = explainer(X_background).values

    # ---------------- TREE MODELS ----------------
    elif any(x in model_name for x in ["Tree", "Forest", "GradientBoosting"]):
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(X_background).values

    # ---------------- KERNEL MODELS (SVC, KNN) ----------------
    else:
        explainer = shap.KernelExplainer(model.predict, X_background)
        shap_values = explainer.shap_values(X_background)

        if isinstance(shap_values, list):
            shap_values = shap_values[0]

    # Plot
    shap.summary_plot(shap_values, X_background, show=True)
    plt.show()

    # ðŸš¨ RETURN DATAFRAME, NOT STRING
    return shap_values, X_background



"""Run AutoML"""

def full_run(data, target_column, preprocess_fn):

    # 1ï¸âƒ£ Detect task FIRST (raw data)
    task = detect_task_type(data, target_column)
    print("Detected task:", task)

    # 2ï¸âƒ£ Preprocess
    X, y, artifacts = preprocess_fn(data, target_column)

    # 3ï¸âƒ£ Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4ï¸âƒ£ AutoML
    results = run_automl(X_train, X_test, y_train, y_test, task)
    print(results)

    # 5ï¸âƒ£ Best model
    best = choose_best_model(results, task)
    model_name = best["model"]

    model = get_models(task)[model_name]
    model.fit(X, y)

    save_model(model, "best_model")

    shap_values, X_shap = explain_model_with_shap(model, X_train)

    print("Best Model:", model_name)

    return {
        "model": model,
        "results": results,
        "X_train": X_train,
        "task": task
    }

"""Normalizing SHAP values"""

def normalize_shap_values(shap_values):
    import numpy as np

    # Case 1: SHAP returns list (classification)
    if isinstance(shap_values, list):
        shap_values = shap_values[0]

    shap_values = np.array(shap_values)

    # Case 2: 3D â†’ collapse outputs
    if shap_values.ndim == 3:
        shap_values = np.mean(shap_values, axis=0)

    return shap_values

"""Run"""

# model, results, shap_values, X_shap = full_run(
#     data,
#     target_column="tip",
#     preprocess_fn=full_preprocessing_pipeline
# )

output = full_run(
    data=data,
    target_column="tip",
    preprocess_fn=full_preprocessing_pipeline
)

model = output["model"]
results = output["results"]
X_train = output["X_train"]
task = output["task"]

#iris dataset
from sklearn.datasets import load_iris

iris=load_iris(as_frame=True)
df_iris = iris.frame
df_iris.head()

output = full_run(df_iris, "target", full_preprocessing_pipeline)

model = output["model"]
X_train = output["X_train"]
task = output["task"]

#diabetes dataset

from sklearn.datasets import load_diabetes

diab = load_diabetes(as_frame=True)
df_diab = diab.frame
df_diab.head()

#Run AutoML
model, results, diabetes_shap_values, X_train = full_run(df_diab, target_column='target', preprocess_fn=full_preprocessing_pipeline)

"""**6. SHAP-DRIVEN DECISION ENGINE**

SHAP Insight Extraction
"""

def extract_shap_insights(shap_values, feature_names, top_k=5):
    import numpy as np

    shap_values = np.array(shap_values)

    # Handle multiclass: (samples, features, classes)
    if shap_values.ndim == 3:
        shap_values = np.mean(np.abs(shap_values), axis=2)

    mean_impact = np.mean(np.abs(shap_values), axis=0)

    insights = []
    for i, feature in enumerate(feature_names):
        insights.append({
            "feature": feature,
            "mean_impact": round(float(mean_impact[i]), 4)
        })

    insights = sorted(insights, key=lambda x: x["mean_impact"], reverse=True)

    return insights[:top_k]

# Run pipeline
output = full_run(data, "tip", full_preprocessing_pipeline)

model = output["model"]
X_train = output["X_train"]

# Run SHAP
shap_values, X_shap = explain_model_with_shap(model, X_train)

# Extract insights
shap_insights = extract_shap_insights(
    shap_values,
    feature_names=X_shap.columns,
    top_k=5
)

shap_insights

"""Decision Agent

"""

def decision_agent(shap_insights, task_type):
  actions = []

  for item in shap_insights:
    feature = item["feature"]
    impact = item["mean_impact"]

    if task_type == "classification":
      if impact > 0.5:
        actions.append(f"Investigate impact of {feature}")
      else:
        if impact > 0.3:
          actions.append(f"Monitor and Optimize {feature}")

  return list(set(actions))

actions = decision_agent(shap_insights, task)
actions

"""Natural Language Explanation

"""

def generate_nlg_explanation(shap_insights, task="regression"):

  """
  Convert SHAP insights into human-readable explanation
  """
  if not shap_insights or len(shap_insights) == 0:
    return "No Significant features were identified in the model explanation."

  explanation = []

  top_feature = shap_insights[0]["feature"]
  explanation.append(f"The model's predictions are mainly influenced by '{top_feature}'.")

  for item in shap_insights[1:3]:
    explanation.append(f"'{item['feature']}' also contributes to the prediction, but to a lesser extent.")

  if task == "regression":
    explanation.append(f"Overall, the model estimates a continuous outcome by combining these feature effects.")

  elif shap_insights[0]["mean_impact"] > 0.5:
    explanation.append(f"This feature has a strong influence on the prediction.")

  else:
    explanation.append(f"Overall, the model uses these features to distinguish between different classes.")

  return " ".join(explanation)

"""End-to-End Intelligent Prediction

"""

nlg_explanation = generate_nlg_explanation(shap_insights, task = task)
print("Model Explanation:")
print(nlg_explanation)